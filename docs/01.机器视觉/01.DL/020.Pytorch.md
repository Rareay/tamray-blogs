---
title: Pytorch
date: 2022-04-03 15:01:03
permalink: /pages/fca7fb/
categories: 
  - 机器视觉
  - DL
tags: 
  - 
---


## 1 安装

环境：ubuntu20 + conda + pip

命令：
```python
pip install torch torchvision torchaudio -i https://pypi.tuna.tsinghua.edu.cn/simple
```


## 2 Tensor 的创建

- 由 numpy 转换
  ```python
  a = np.array([1,2,3])
  b = torch.from_numpy(a)
  ```

- torch 直接创建
  ```python
  a = torch.tensor([[1,2,3],[4,5,6]])      # 数据类型 int64
  a = torch.IntTensor([[1,2,3],[4,5,6]])   # 数据类型 int32
  a = torch.FloatTensor([[1,2,3],[4,5,6]]) # 数据类型 float32
  ```
- tensor 数据类型的转换
  ```python
  tensor = torch.Tensor(2, 5)
  torch.char()          # 将 tensor 投射为  int8
  torch.byte()          # 将 tensor 投射为 uint8
  torch.short()         # 将 tensor 投射为  int16
  torch.int()           # 将 tensor 投射为  int32
  torch.long()          # 将 tensor 投射为  int64
  torch.half()          # 将 tensor 投射为  float16
  torch.float()         # 将 tensor 投射为  float32
  torch.double()        # 将 tensor 投射为  float64
  ```
- 随机数
  ```python
  a = torch.rand(3,3)    # 取 0 ~ 1 之间的随机数
  a = torch.rand_like(a) #  创建一个 shape 与 a 相同的 tensor
  a = torch.randint(1, 10, (3, 3)) # [0, 10) 区间内的随机数
  ```
- 正太分布
  ```python
  a = torch.randn(3,3) # 正态分布（0，1）
  ```
- 全 0、全 1、全 n、对角矩阵
  ```python
  a = torch.ones(3, 3)  # 全 1
  a = torch.zeros(3, 3) # 全 0 
  a = torch.full([3, 3], 100) # 全 100
  a = torch.eye(3, 3)   # 对角矩阵
  ```
- 线性分布
  ```python
  a = torch.arange(0, 10, 2)         # [0, 10)，步长为 2
  # 输出 tensor([0, 2, 4, 6, 8])
  a = torch.arange(10)               # [0, 10)，步长为 1
  a = torch.range(10)                # [0, 10]，步长为 1

  a = torch.linspace(0, 10, steps=5) # 从 0 到 10，平分为 5 份
  # 输出 tensor([ 0.0000,  2.5000,  5.0000,  7.5000, 10.0000])
  ```
- tensor 索引
  ```python
  a = torch.randn(5, 3, 28, 28)

  b = a[:, :, 0:28, 0:20:2]  # : 表示所有，0:28 表示 0~27，0:28:2 表示 0~27，步长为 2

  c = a.index_select(2, torch.tensor([0,1])) # 根据第 2 维的index，筛选出 [0, 1] 的数据
  # 等同于 a[:, :, [0, 1], :]
  ```
- ... 表示所有
  ```python
  a = torch.randn(5, 3, 28, 28)

  b = a[..., 0:28]
  # 等同于 a[:, :, :, 0:28]
  ```
- reshape 更改维度
  用于改变数据的维度，是否穿件新的数据区存储空间有两种情况：
  ==a. 若要操作的 tensor 数据区为连续分布，则不创建，reshape 后指向原数据区；==
  ==b. 若要操作的 tensor 数据区不连续（被permut、transpose操作过），则创建新的数据区；==
  ```python
  a = torch.rand(1,2,3,3)
  b = a.reshape( 6, 3) # 将 shape 1*2*3*3 改成了 6*3
  b = a.reshpae(-1, 3) # -1 表示该维度的大小自动计算，当然，不能同时出现两个 -1
  ```
- view 更改维度
  不创建新数据区存储空间，更改维度（的描述）。
  **注意：操作的 tensor 需要是数据连续分布，如果 tensor 使用 permut()、transpose() 操作过，数据区就不是连续分布的了，不能使用 view()，但可以使用 reshape()。**
  ```python
  a = torch.rand(1,2,3,3)
  b = a.view( 6, 3) # 将 shape 1*2*3*3 改成了 6*3
  b = a.view(-1, 3) # -1 表示该维度的大小自动计算，当然，不能同时出现两个 -1
  ```
- permute 重新排列维度（会改变数据分布）
  ```python
  a = torch.tensor(2,3,4)

  b = a.permute(2,1,0) # 维度排列从原来的 0,1,2 改成 2,1,0
  # shape: [4,3,2]
  ```
- transpose 交换两个维度
  ```python
  a = torch.tensor(2,3,4)

  b = a.transpose(0, 1) # 交换第 0 维和第 1 维
  # shape: [3,2,4]
  ```
- unsqueeze 增加1维度、squeeze 去掉1维度
  不改变数据存储空间，更改维度（的描述）
  ```python
  a = torch.rand(2,3)

  b = a.unsqueeze(1) # 在第 1 维处（0维后面）增加一维
  # shape 变成：[2,1,3]

  c = b.squeeze(1)  # 去掉第 1 维（前提是该处维数为1，否则不生效）
  # shape 变为：[2,3]
  ```
- expand 扩展维度
  **注意：expand 不改变数据存储空间，虽然增加了维数，但新增的维数是指针，指向原来的数据区；并且，要扩展的维数只能是 1，大于 1 的维数不能扩展。**
  ```python
  a = torch.tensor([[0, 1, 2]])
  # a:
  #   tensor([[0, 1, 2]])

  b = a.expand(2, 3) # 维数从原来的 1*3 扩展到 2*3
  #   tensor([[0, 1, 2],
  #           [0, 1, 2]])
  # 这两行 [0, 1, 2] 的内存指向同一个地址，更改其中一个数据，另一个也会改变

  c = a.expand_as(b) # 扩展的 shape 和 b 保持一致
  ```
- repeat 扩展数据（同时扩展维度）
  存储空间会发生改变，有复制数据的操作。
  ```python
  a = torch.tensor([0, 1, 2])
  # a:
  #   tensor([0, 1, 2])

  b = a.repeat(3, 2) # 表示对应维度重复的次数
  #   tensor([[0, 1, 2, 0, 1, 2],
  #           [0, 1, 2, 0, 1, 2],
  #           [0, 1, 2, 0, 1, 2]])
  ```
- cat 拼接维数
  ```python
  a = torch.rand(1,3,3)
  b = torch.rand(1,3,3)
  c = torch.cat([a,b], 0) # 在第 0 维上拼接
  # shape: [2,3,3]
  ```
# 3 Tensor 的运算

- 乘法
  ```python
  torch.mm(a, b)     # 矩阵乘法
  a * b              # 矩阵对应元素相乘
  a * 3              # 矩阵所有元素乘以 3
  ```
- 加法、减法、除法
  ```python
  a + b # 对应元素相加
  a + 2 # 所有元素加 2
  ```
- $n^x$ 和 $\sqrt{x}$
  ```python
  a**2      # 幂
  a.rsqrt() # 平方
  ```
- $e^x$ 和 $log(x)$ 
  ```python
  torch.exp(a)
  torch.log(a)
  ```
- 截断
  ```python
  a.clamp(4)     # 元素小于 4 的都等于 4
  a.clamp(4, 10) # 元素小于 4 的都等于 4，大于 10 的都等于 10
  ```
- 四舍五入
  ```python
  a.round() # float 的四舍五入
  ```
- 向上取整
  ```python
  a.ceil()  # float 的
  ```
- 绝对值
  ```python
  a.abs(0)
  ```
- 最大、最小值
  ```python
  a.max() # 返回矩阵中的最大值
  values, indices = a.max(0) # 返回指定维度最大值

  a.argmax() # 返回最大值的索引
  a.argmax(dim=0) # 返回指定维度最大值的索引

  a.min() # 同上
  a.argmin()
  ```
- 均值、求和
  ```python
  a.mean()
  a.mean(dim=0) # 指定维度的均值
  a.sun()
  a.sun(dmi=0)
  ```
- 范数
  $||x|| = (|x_1|^p + |x_2|^p +...+ |x_n|^p)^{\frac{1}{p}}$
  ```python
  a.norm()  # 默认 p = 2
  a.norm(3) # p = 3
  ```
- 统计 TOPn
  ```python
  a = torch.rand(10,3,3)
  values, indeces = a.topk(2, dim=0) # 指定维度中，统计 TOP2 的数据
  # shape： [2, 3, 3]
  ```
- 判断相等、不相等
  ```python
  torch.eq(a, b)   # 判断对应元素相等，返回 bool
  torch.eq(a, 0)   # 判断与 1 相等，返回 bool
  torch.equal(a,b) # 判断不等
  ```


## 自定义损失函数

损失函数是描述变量（output）和目标值（target）的差异，反向求导会根据其梯度自动优化，直到梯度为0，所以构造损失函数的时候也要确保梯度为 0 的时候，loss 也为 0，例如：$loss = (output - target)^2$，示例代码：
```python
import torch
import torch.nn as nn

class my_loss(nn.Module):
    def __init__(self) -> None:
        super().__init__()
    
    def forward(self, x, label):
        l = torch.zeros_like(x) # 自己创建的tensor默认不求梯度，即requires_grad=False
        l[0] = (x[0] - label[0])**2
        l[1] = (x[1] - label[1] - 3)**2
        return l.sum()

x = torch.Tensor([0.12, 0.34])
x.requires_grad = True # 设置tensor可自动求梯度
lable = torch.Tensor([10, 20])

optimizer = torch.optim.Adam([x], lr=0.1)
loss_fun = my_loss()

for i in range(1000):
    pre = x ** 2
    loss = loss_fun(pre, lable)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print(pre)
    print(lable)
    print(loss)
    '''
    最终输出：
    tensor([10.0000, 23.0000], grad_fn=<PowBackward0>)
    tensor([10., 20.])
    tensor(3.6380e-12, grad_fn=<SumBackward0>)
    '''
```



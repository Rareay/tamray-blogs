---
title: tensorflow
date: 2020-08-28 15:53:35
permalink: /pages/5c3ab3/
categories: 
  - 编程语言
  - Python
tags: 
  - tensorflow
  - python
---
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https'){
   bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else{
  bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>




## 1 sess会话

```python
with tf.Session() as sess:
	sess.run(tf.global_variables_initializer())#如果前面创建了变量
	sess.run(变量) # 运算某个变量 
	sess.run(变量,feed_dic={参数1:xxx, 参数2:xxx}) # 如果要运算的变量需要对用的传入参数
```

## 2 变量/常量的创建、使用

```python
tf.Variable(n) #创建变量n
tf.assign(a, n) #给变量n赋值为n，变量只能通过该方法完成赋值
tf.constant(n) #创建常量n
tf.assign(n1, n2) #给变量赋值，n1=n2
tf.initialize_all_variables() #初始化创建的变量
input = tf.placeholder(tf.float32) #创建占位符，像是创建输入变量，float32是数据类型
sess.run(xxx, feed_dict={input1:[2.], input2:[3.]}) #给占位符赋值
tf.random_normal([n1, n2]) #从正太分布中取随机数，生成二维矩阵
tf.zeros([n1, n2]) #生成二维零矩阵
tf.train.GradientDescentOptimizer(0.1).minimize(loss) #随机梯度下降算法，学习率为0.1
```
 
## 3 运算
```python
tf.add(n1, n2) # 加法运算
tf.multiply(n1 ,n2) # 对应元素相乘
tf.matmul(n1 ,n2) # 点乘
tf.reduce_sum(X) # 对矩阵所有元素求和
tf.reduce_sum(X, 0) # 0表示对矩阵元素按列求和，1表示对矩阵元素按行求和
tf.reduce_mean(X) # 对矩阵所有元素求平均值
tf.reduce_mean(X, 0) # 0表示对矩阵元素按列平均值，1表示对矩阵元素按行平均值
tf.subtract(x, y) # 张量相减，x - y 
tf.pow(x, y) # 逐元素求幂 
tf.exp(x) # 等价于 pow(e, x)，其中 e 是欧拉数（2.718…） 
tf.sqrt(x) # 等价于 pow(x, 0.5) 
tf.div(x, y) # 两个张量逐元素相除 
tf.truediv(x, y) # 与tf.div 相同，只是将参数强制转换为 float 
tf.floordiv(x, y) # 与truediv 相同，只是将结果舍入为整数
tf.mod(x, y) # 逐元素取余
tf.equal(A, B) # 比较矩阵，返回值为布尔型
tf.cast(a,dtype=tf.bool) # 将float型转换成bool型，dtype=tf.float32表示转换成float型
```
 
## 4 tensorboard 的使用
1） 开始/结束记录计算图
```python
writer = tf.summary.FileWriables("logs/", sess.graph) # "logs/"是log保存的路径
writer.close()
```
2） 跟踪某个变量
```python
tf.summary.histogram("name", 变量)
tf.summary.scalar("name", 变量)
```
3） 汇总跟踪的变量
```python
merged = tf.summary.merge_all()
```
4） 为summery增加维度
```python
writer.add_summary(merged, i) #此处的i就是增加的维度，是绘图需要
```
5） 在计算图中给块命名
```python
with tf.name_scope("name")
```
 
## 5 卷积（CNN）  

- 输入数据：
4维的张量，输入向量可以用tf.reshape(x,shape)转换成4维，，shape=[x x x x]
这4维：
```python
[batch, in_height, in_width, in_channels]
[训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]
```

- 卷积核：
4维的张量，可以由tf.truncated_normal(shape, stddev=0.1) 创建，shape=[x x x x]
这4维：
```python
[filter_height, filter_width, in_channels, out_channels]
[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]
```

- 卷积：
```python
tf.nn.conv2d(数据， 卷积核, strides=[1,1,1,], padding="VALID")
# strides: 步长
# padding: 可选‘VALID’和‘SAME’，VALID表示不扩展边缘，卷积后尺寸会变小，SAME会扩展边缘，卷积后尺寸不变
```

## 6 relu激活
```python
tf.nn.relu(数据) # 数据一般为卷积+偏置
```

## 7 池化
```python
max_pool_2x2(数据) # 2x2的池化
```

## 8 反卷积
```python
tf.layers.conv2d_transpose(h0, kernel_size=3, filters=256, strides=2, padding='same')
# h0：原始层的数据
# kernel_size：反卷积核的尺寸
# filters：反卷积核的种类，即特征数
# strides：步长
# padding：反卷积的模式，分 same 和 valid 两种
```

1） 输入层尺寸为 2x2， kernel_size=3, strides=2, padding='same' 时：

<img src='/pic/031.png' width='100%'/>

2）输入层尺寸为 2x2， kernel_size=3, strides=2, padding='valid' 时：

<img src='/pic/032.png' width='100%'/>

## 7 tf.nn.dropout()
```python
tf.nn.dropout(x, keep_prob,noise_shape=None, seed=None, name=None)
# x: 要训练的数据
# keep_prob: dropout概率，一般使用占位符。
#说明：dropout就是使输入tensor中某些元素变为0，变0的概率为1-keep_prob，其它没变0的元素变为原来的1/keep_prob大小！
```

## 9 tf.variable_scope 与 tf.name_scope 的区别
共同点：都是用于管理变量组名称
不同点：tf.variable_scope 后面可以有 tf.get_variable 和 tf.Variable，而 tf.name_scope 后面只能有 tf.Variable
如：
```python
with tf.variable_scope('V1'):  
    a1 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1))  
    a2 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2')  

with tf.name_scope('V2'):  
    # a1 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1))  这里不能用
    a2 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2')  
```
 

## 10 保存/加载训练模型
1） 保存模型：
```python
# 下面这行需要放在函数外面
saver = tf.train.Saver([w1,w2]， max_to_keep=4，keep_checkpoint_every_n_hours=2)
# [w1,w2]是需要保存的参数，若不指定，默认保存所有
# max_to_keep是保存模型的个数，若保存了很多次，那么只存最新的那几个
# keep_checkpoint_every_n_hours表示多少小时自动保存一次
with tf.Session() as sess:
	sess.run(tf.global_variables_initializer()) 
	saver.save(sess, "model/my-model", global_step=epoch)
```

2） 加载模型：
```python
with tf.Session() as sess:
	saver =tf.train.import_meta_graph('model/my-model.meta')
    saver.restore(sess,tf.train.latest_checkpoint("model/"))
```
3） 演示程序
该演示程序可以随时中断，再次运行可以继续训练
```python
import tensorflow as tf

train_num = tf.Variable(0)
train_num = tf.assign(train_num, train_num+1, name='train_num')

saver = tf.train.Saver() # 需要放在函数外面
def run():
    with tf.Session() as sess:
        try:
            saver2 = tf.train.import_meta_graph('model2/my-model.meta')
            saver2.restore(sess, tf.train.latest_checkpoint("model2/"))
            print("加载模型...")
        except Exception:
            print("加载模型出错!")
            print("重新开始训练模型...")
            sess.run(tf.global_variables_initializer())
        for i in range(100):
            temp = sess.run(train_num)
            print("第", temp , "次训练")
            saver.save(sess, "model2/my-model")
            #saver.save(sess, "model2/my-model", global_step=i)

if __name__ == "__main__":
    run()
```

## 11 激活函数


1） sigmoid函数
```python
tf.sigmoid()
```

2） Tanh函数
```python
tf.tanh()
```

3） ReLU函数
```python
tf.nn.relu()
```

4） Softplus函数
```python
tf.nn.softplus()
```
5） Softmax函数
```python
tf.nn.softmax()
```
 
## 12 TensorBoard
1） 函数

```python
#记录标量数据
tf.summary.scalar('xxx_name', xxx)

#记录直方图数据
tf.summary.histogram('xxx_name', xxx)

#记录文本输入
tf.summary.text('xxx_name', xxx)

#记录图像数据
tf.summary.image('xxx_name', xxx)

#记录音频数据
tf.summary.audio('xxx_name', xxx)

#将上面所有的summary信息合并
tf.summary.merge_all()

#将信息保存到文件中，在终端执行： tensorboard --logdir 路径
tf.summary.FileWriter('保存的路径', sess.graph)
```

2） 参考代码
```python
import tensorflow as tf
import time

train_num = tf.Variable(0)
tf.summary.scalar('name', train_num) # 记录该标量数据
add = tf.assign(train_num, train_num+1, name='add_option') # 让train_num加1

with tf.Session() as sess:
		merge_all = tf.summary.merge_all() # 将所有的summary信息合并
		writer = tf.summary.FileWriter('./logs/test', sess.graph) # 将summary和图保存到文件中
		sess.run(tf.global_variables_initializer())
		i = 1
		while i == 1:
			i += 1
			merge_result ,_ =sess.run([merge_all, add]) # 去计算merge_all从而合并信息
			writer.add_summary(merge_result, i) # 将新的信息添加到summary中
			time.sleep(2) # 延时便于观测
		writer.close()
```

